<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://hehelv.github.io/</id>
    <title>💤ISpiker</title>
    <updated>2020-02-28T06:26:44.247Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://hehelv.github.io/"/>
    <link rel="self" href="https://hehelv.github.io//atom.xml"/>
    <subtitle>The proof is in the pudding.</subtitle>
    <logo>https://hehelv.github.io//images/avatar.png</logo>
    <icon>https://hehelv.github.io//favicon.ico</icon>
    <rights>All rights reserved 2020, 💤ISpiker</rights>
    <entry>
        <title type="html"><![CDATA[1-http协议与chrome抓包工具]]></title>
        <id>https://hehelv.github.io//post/1-http-xie-yi-yu-chrome-zhua-bao-gong-ju</id>
        <link href="https://hehelv.github.io//post/1-http-xie-yi-yu-chrome-zhua-bao-gong-ju">
        </link>
        <updated>2020-02-28T06:20:21.000Z</updated>
        <content type="html"><![CDATA[<h1 id="2-http协议和chrome抓包工具">2-http协议和Chrome抓包工具</h1>
<h2 id="什么是http和https协议">什么是http和https协议：</h2>
<p>HTTP协议：全称是<code>HyperText Transfer Protocol</code>，中文意思是超文本传输协议，是一种发布和接收HTML页面的方法。服务器端口号是<code>80</code>端口。 HTTPS协议：是HTTP协议的加密版本，在HTTP下加入了SSL层。服务器端口号是<code>443</code>端口。</p>
<h2 id="在浏览器中发送一个http请求的过程">在浏览器中发送一个http请求的过程：</h2>
<ol>
<li>当用户在浏览器的地址栏中输入一个URL并按回车键之后，浏览器会向HTTP服务器发送HTTP请求。HTTP请求主要分为“Get”和“Post”两种方法。</li>
<li>当我们在浏览器输入URL http://www.baidu.com 的时候，浏览器发送一个Request请求去获取 http://www.baidu.com 的html文件，服务器把Response文件对象发送回给浏览器。</li>
<li>浏览器分析Response中的 HTML，发现其中引用了很多其他文件，比如Images文件，CSS文件，JS文件。 浏览器会自动再次发送Request去获取图片，CSS文件，或者JS文件。</li>
<li>当所有的文件都下载成功后，网页会根据HTML语法结构，完整的显示出来了。</li>
</ol>
<h2 id="url详解">url详解：</h2>
<p><code>URL</code>是<code>Uniform Resource Locator</code>的简写，统一资源定位符。 一个<code>URL</code>由以下几部分组成：</p>
<pre><code>    scheme://host:port/path/?query-string=xxx#anchor
</code></pre>
<ul>
<li><strong>scheme</strong>：代表的是访问的协议，一般为<code>http</code>或者<code>https</code>以及<code>ftp</code>等。</li>
<li><strong>host</strong>：主机名，域名，比如<code>www.baidu.com</code>。</li>
<li><strong>port</strong>：端口号。当你访问一个网站的时候，浏览器默认使用80端口。</li>
<li><strong>path</strong>：查找路径。比如：<code>www.jianshu.com/trending/now</code>，后面的<code>trending/now</code>就是<code>path</code>。</li>
<li><strong>query-string</strong>：查询字符串，比如：<code>www.baidu.com/s?wd=python</code>，后面的<code>wd=python</code>就是查询字符串。</li>
<li><strong>anchor</strong>：锚点，后台一般不用管，前端用来做页面定位的。</li>
</ul>
<p>在浏览器中请求一个<code>url</code>，浏览器会对这个url进行一个编码。除英文字母，数字和部分符号外，其他的全部使用百分号+十六进制码值进行编码。</p>
<h2 id="常用的请求方法">常用的请求方法：</h2>
<p>在<code>Http</code>协议中，定义了八种请求方法。这里介绍两种常用的请求方法，分别是<code>get</code>请求和<code>post</code>请求。</p>
<ol>
<li><code>get</code>请求：一般情况下，只从服务器获取数据下来，并不会对服务器资源产生任何影响的时候会使用<code>get</code>请求。</li>
<li><code>post</code>请求：向服务器发送数据（登录）、上传文件等，会对服务器资源产生影响的时候会使用<code>post</code>请求。 以上是在网站开发中常用的两种方法。并且一般情况下都会遵循使用的原则。但是有的网站和服务器为了做反爬虫机制，也经常会不按常理出牌，有可能一个应该使用<code>get</code>方法的请求就一定要改成<code>post</code>请求，这个要视情况而定。</li>
</ol>
<h2 id="请求头常见参数">请求头常见参数：</h2>
<p>在<code>http</code>协议中，向服务器发送一个请求，数据分为三部分，第一个是把数据放在url中，第二个是把数据放在<code>body</code>中（在<code>post</code>请求中），第三个就是把数据放在<code>head</code>中。这里介绍在网络爬虫中经常会用到的一些请求头参数：</p>
<ol>
<li><code>User-Agent</code>：浏览器名称。这个在网络爬虫中经常会被使用到。请求一个网页的时候，服务器通过这个参数就可以知道这个请求是由哪种浏览器发送的。如果我们是通过爬虫发送请求，那么我们的<code>User-Agent</code>就是<code>Python</code>，这对于那些有反爬虫机制的网站来说，可以轻易的判断你这个请求是爬虫。因此我们要经常设置这个值为一些浏览器的值，来伪装我们的爬虫。</li>
<li><code>Referer</code>：表明当前这个请求是从哪个<code>url</code>过来的。这个一般也可以用来做反爬虫技术。如果不是从指定页面过来的，那么就不做相关的响应。</li>
<li><code>Cookie</code>：<code>http</code>协议是无状态的。也就是同一个人发送了两次请求，服务器没有能力知道这两个请求是否来自同一个人。因此这时候就用<code>cookie</code>来做标识。一般如果想要做登录后才能访问的网站，那么就需要发送<code>cookie</code>信息了。</li>
</ol>
<h2 id="常见响应状态码">常见响应状态码：</h2>
<ol>
<li><code>200</code>：请求正常，服务器正常的返回数据。</li>
<li><code>301</code>：永久重定向。比如在访问<code>www.jingdong.com</code>的时候会重定向到<code>www.jd.com</code>。</li>
<li><code>302</code>：临时重定向。比如在访问一个需要登录的页面的时候，而此时没有登录，那么就会重定向到登录页面。</li>
<li><code>400</code>：请求的<code>url</code>在服务器上找不到。换句话说就是请求<code>url</code>错误。</li>
<li><code>403</code>：服务器拒绝访问，权限不够。</li>
<li><code>500</code>：服务器内部错误。可能是服务器出现<code>bug</code>了。</li>
</ol>
<h2 id="chrome抓包工具">Chrome抓包工具：</h2>
<p><code>Chrome</code>浏览器是一个非常亲近开发者的浏览器。可以方便的查看网络请求以及发送的参数。对着网页<code>右键-&gt;检查</code>。然后就可以打开开发者选项。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[TCPIP详解]]></title>
        <id>https://hehelv.github.io//post/tcpip-xiang-jie</id>
        <link href="https://hehelv.github.io//post/tcpip-xiang-jie">
        </link>
        <updated>2020-02-20T06:49:46.000Z</updated>
        <content type="html"><![CDATA[<h1 id="2-beautifulsoup4库">2-BeautifulSoup4库</h1>
<p>和 lxml 一样，Beautiful Soup 也是一个HTML/XML的解析器，主要的功能也是如何解析和提取 HTML/XML 数据。<br>
lxml 只会局部遍历，而Beautiful Soup 是基于HTML DOM（Document Object Model）的，会载入整个文档，解析整个DOM树，因此时间和内存开销都会大很多，所以性能要低于lxml。<br>
BeautifulSoup 用来解析 HTML 比较简单，API非常人性化，支持CSS选择器、Python标准库中的HTML解析器，也支持 lxml 的 XML解析器。<br>
Beautiful Soup 3 目前已经停止开发，推荐现在的项目使用Beautiful Soup 4。</p>
<h2 id="安装和文档">安装和文档：</h2>
<ol>
<li>安装：<code>pip install bs4</code>。</li>
<li>中文文档：https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html</li>
</ol>
<h2 id="几大解析工具对比">几大解析工具对比：</h2>
<table>
<thead>
<tr>
<th>解析工具</th>
<th>解析速度</th>
<th>使用难度</th>
</tr>
</thead>
<tbody>
<tr>
<td>BeautifulSoup</td>
<td>最慢</td>
<td>最简单</td>
</tr>
<tr>
<td>lxml</td>
<td>快</td>
<td>简单</td>
</tr>
<tr>
<td>正则</td>
<td>最快</td>
<td>最难</td>
</tr>
</tbody>
</table>
<h2 id="简单使用">简单使用：</h2>
<pre><code class="language-python">from bs4 import BeautifulSoup

html = &quot;&quot;&quot;
&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;
&lt;body&gt;
&lt;p class=&quot;title&quot; name=&quot;dromouse&quot;&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;
&lt;p class=&quot;story&quot;&gt;Once upon a time there were three little sisters; and their names were
&lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,
&lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and
&lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;;
and they lived at the bottom of a well.&lt;/p&gt;
&lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;
&quot;&quot;&quot;

#创建 Beautiful Soup 对象
# 使用lxml来进行解析
soup = BeautifulSoup(html,&quot;lxml&quot;)

print(soup.prettify())
</code></pre>
<h2 id="四个常用的对象">四个常用的对象：</h2>
<p>Beautiful Soup将复杂HTML文档转换成一个复杂的树形结构,每个节点都是Python对象,所有对象可以归纳为4种:</p>
<ol>
<li>Tag</li>
<li>NavigatableString</li>
<li>BeautifulSoup</li>
<li>Comment</li>
</ol>
<h3 id="1-tag">1. Tag：</h3>
<p>Tag 通俗点讲就是 HTML 中的一个个标签。示例代码如下：</p>
<pre><code class="language-python">(from bs4 import BeautifulSoup)

html = &quot;&quot;&quot;
&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;
&lt;body&gt;
&lt;p class=&quot;title&quot; name=&quot;dromouse&quot;&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;
&lt;p class=&quot;story&quot;&gt;Once upon a time there were three little sisters; and their names were
&lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,
&lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and
&lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;;
and they lived at the bottom of a well.&lt;/p&gt;
&lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;
&quot;&quot;&quot;

#创建 Beautiful Soup 对象
soup = BeautifulSoup(html,'lxml')


print (soup.title)
# &lt;title&gt;The Dormouse's story&lt;/title&gt;

print (soup.head)
# &lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;

print (soup.a)
# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;

print( soup.p)
# &lt;p class=&quot;title&quot; name=&quot;dromouse&quot;&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;

print (type(soup.p))
# &lt;class 'bs4.element.Tag'&gt;
</code></pre>
<p>我们可以利用 soup 加标签名轻松地获取这些标签的内容，这些对象的类型是bs4.element.Tag。但是注意，它查找的是在所有内容中的第一个符合要求的标签。如果要查询所有的标签，后面会进行介绍。<br>
对于Tag，它有两个重要的属性，分别是name和attrs。示例代码如下：</p>
<pre><code class="language-python">print soup.name
# [document] #soup 对象本身比较特殊，它的 name 即为 [document]

print soup.head.name
# head #对于其他内部标签，输出的值便为标签本身的名称

print soup.p.attrs
# {'class': ['title'], 'name': 'dromouse'}
# 在这里，我们把 p 标签的所有属性打印输出了出来，得到的类型是一个字典。

print soup.p['class'] # soup.p.get('class')
# ['title'] #还可以利用get方法，传入属性的名称，二者是等价的

soup.p['class'] = &quot;newClass&quot;
print soup.p # 可以对这些属性和内容等等进行修改
# &lt;p class=&quot;newClass&quot; name=&quot;dromouse&quot;&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;
</code></pre>
<h3 id="2-navigablestring">2. NavigableString：</h3>
<p>如果拿到标签后，还想获取标签中的内容。那么可以通过<code>tag.string</code>获取标签中的文字。示例代码如下：</p>
<pre><code class="language-python">print (soup.p.string)
# The Dormouse's story

print (type(soup.p.string))
# &lt;class 'bs4.element.NavigableString'&gt;thon
</code></pre>
<h3 id="3-beautifulsoup">3. BeautifulSoup：</h3>
<p>BeautifulSoup 对象表示的是一个文档的全部内容.大部分时候,可以把它当作 Tag 对象,它支持 遍历文档树 和 搜索文档树 中描述的大部分的方法.<br>
因为 BeautifulSoup 对象并不是真正的HTML或XML的tag,所以它没有name和attribute属性.但有时查看它的 .name 属性是很方便的,所以 BeautifulSoup 对象包含了一个值为 “[document]” 的特殊属性 .name</p>
<pre><code class="language-python">soup.name
# '[document]'
</code></pre>
<h3 id="4-comment">4. Comment：</h3>
<p>Tag , NavigableString , BeautifulSoup 几乎覆盖了html和xml中的所有内容,但是还有一些特殊对象.容易让人担心的内容是文档的注释部分:</p>
<pre><code class="language-python">markup = &quot;&lt;b&gt;&lt;!--Hey, buddy. Want to buy a used parser?--&gt;&lt;/b&gt;&quot;
soup = BeautifulSoup(markup)
comment = soup.b.string
type(comment)
# &lt;class 'bs4.element.Comment'&gt;
</code></pre>
<p>Comment 对象是一个特殊类型的 NavigableString 对象:</p>
<pre><code class="language-python">comment
# 'Hey, buddy. Want to buy a used parser'
</code></pre>
<h2 id="遍历文档树">遍历文档树：</h2>
<h3 id="1-contents和children">1. contents和children：</h3>
<pre><code class="language-python">html_doc = &quot;&quot;&quot;
&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;

&lt;p class=&quot;title&quot;&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;

&lt;p class=&quot;story&quot;&gt;Once upon a time there were three little sisters; and their names were
&lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;,
&lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and
&lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;;
and they lived at the bottom of a well.&lt;/p&gt;

&lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;
&quot;&quot;&quot;

from bs4 import BeautifulSoup
soup = BeautifulSoup(html_doc,'lxml')

head_tag = soup.head
# 返回所有子节点的列表
print(head_tag.contents)

# 返回所有子节点的迭代器
for child in head_tag.children:
    print(child)
</code></pre>
<h3 id="2-strings-和-stripped_strings">2. strings 和 stripped_strings</h3>
<p>如果tag中包含多个字符串 [2] ,可以使用 .strings 来循环获取：</p>
<pre><code class="language-python">for string in soup.strings:
    print(repr(string))
    # u&quot;The Dormouse's story&quot;
    # u'\n\n'
    # u&quot;The Dormouse's story&quot;
    # u'\n\n'
    # u'Once upon a time there were three little sisters; and their names were\n'
    # u'Elsie'
    # u',\n'
    # u'Lacie'
    # u' and\n'
    # u'Tillie'
    # u';\nand they lived at the bottom of a well.'
    # u'\n\n'
    # u'...'
    # u'\n'
</code></pre>
<p>输出的字符串中可能包含了很多空格或空行,使用 .stripped_strings 可以去除多余空白内容：</p>
<pre><code class="language-python">for string in soup.stripped_strings:
    print(repr(string))
    # u&quot;The Dormouse's story&quot;
    # u&quot;The Dormouse's story&quot;
    # u'Once upon a time there were three little sisters; and their names were'
    # u'Elsie'
    # u','
    # u'Lacie'
    # u'and'
    # u'Tillie'
    # u';\nand they lived at the bottom of a well.'
    # u'...'
</code></pre>
<h3 id="获取标签属性">获取标签属性</h3>
<p>1.通过下标获取</p>
<pre><code class="language-python">href = a['href']
</code></pre>
<p>2.通过attrs属性获取</p>
<pre><code class="language-python">href=a.attrs['href']
</code></pre>
<h3 id="小结">小结</h3>
<ol>
<li>string: 获取某个标签下的非标签字符串。返回来的是个字符串。</li>
<li>strings: 获取某个标签下的子孙非标签字符串。返回来的是个生成器。</li>
<li>stripped_strings:获取某个标签下的子孙非标签字符串，会去掉空白字符。返回来的<br>
是个生成器。</li>
<li>get_ _text:获取某个标签下的子孙非标签字符串。不是以列表的形式返回，是以普通字<br>
符串返回。</li>
</ol>
<h2 id="搜索文档树">搜索文档树：</h2>
<h3 id="1-find和find_all方法">1. find和find_all方法：</h3>
<p>搜索文档树，一般用得比较多的就是两个方法，一个是<code>find</code>，一个是<code>find_all</code>。<code>find</code>方法是找到第一个满足条件的标签后就立即返回，只返回一个元素。<code>find_all</code>方法是把所有满足条件的标签都选到，然后返回回去。使用这两个方法，最常用的用法是出入<code>name</code>以及<code>attr</code>参数找出符合要求的标签。</p>
<pre><code class="language-python">soup.find_all(&quot;a&quot;,attrs={&quot;id&quot;:&quot;link2&quot;})
</code></pre>
<p>或者是直接传入属性的的名字作为关键字参数：</p>
<pre><code class="language-python">soup.find_all(&quot;a&quot;,id='link2')
</code></pre>
<h3 id="2-select方法">2. select方法：</h3>
<p>使用以上方法可以方便的找出元素。但有时候使用<code>css</code>选择器的方式可以更加的方便。使用<code>css</code>选择器的语法，应该使用<code>select</code>方法。以下列出几种常用的<code>css</code>选择器方法：</p>
<h4 id="1通过标签名查找">（1）通过标签名查找：</h4>
<pre><code class="language-python">print(soup.select('a'))
</code></pre>
<h4 id="2通过类名查找">（2）通过类名查找：</h4>
<p>通过类名，则应该在类的前面加一个<code>.</code>。比如要查找<code>class=sister</code>的标签。示例代码如下：</p>
<pre><code class="language-python">print(soup.select('.sister'))
</code></pre>
<h4 id="3通过id查找">（3）通过id查找：</h4>
<p>通过id查找，应该在id的名字前面加一个＃号。示例代码如下：</p>
<pre><code class="language-python">print(soup.select(&quot;#link1&quot;))
</code></pre>
<h4 id="4组合查找">（4）组合查找：</h4>
<p>组合查找即和写 class 文件时，标签名与类名、id名进行的组合原理是一样的，例如查找 p 标签中，id 等于 link1的内容，二者需要用空格分开：</p>
<pre><code class="language-python">print(soup.select(&quot;p #link1&quot;))
</code></pre>
<p>直接子标签查找，则使用 &gt; 分隔：</p>
<pre><code class="language-python">print(soup.select(&quot;head &gt; title&quot;))
</code></pre>
<h4 id="5通过属性查找">（5）通过属性查找：</h4>
<p>查找时还可以加入属性元素，属性需要用中括号括起来，注意属性和标签属于同一节点，所以中间不能加空格，否则会无法匹配到。示例代码如下：</p>
<pre><code class="language-python">print(soup.select('a[href=&quot;http://example.com/elsie&quot;]'))
</code></pre>
<h4 id="6获取内容">（6）获取内容</h4>
<p>以上的 select 方法返回的结果都是列表形式，可以遍历形式输出，然后用 get_text() 方法来获取它的内容。</p>
<pre><code class="language-python">soup = BeautifulSoup(html, 'lxml')
print type(soup.select('title'))
print soup.select('title')[0].get_text()

for title in soup.select('title'):
    print title.get_text()
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[八皇后]]></title>
        <id>https://hehelv.github.io//post/ba-huang-hou</id>
        <link href="https://hehelv.github.io//post/ba-huang-hou">
        </link>
        <updated>2020-01-18T13:47:19.000Z</updated>
        <content type="html"><![CDATA[<h1 id="代码">代码：</h1>
<pre><code class="language-c++">#include&lt;bits/stdc++.h&gt;
using namespace std;

int n,a[50],total;
int b[100],c[100],d[100];//b,c,d分别代表列，主对角线，副对角线，初始值为0，当有一个皇后出现时，需要将其所占的列，主，副对角线标记为1

void dfs(int i)//这里i代表的是第i个皇后
{
    if(i&gt;n)//i&gt;n时说明已经有了一种解法
    {
        total++;
        if(total&lt;=3)
        {
            for(int j=1;j&lt;=n;j++)
            {
                printf(&quot;%d &quot;,a[j]);
            }
            printf(&quot;\n&quot;);
        }
        return;
    }
    for(int j=1;j&lt;=n;j++)
    {
        if(!b[j]&amp;&amp;!c[i+j]&amp;&amp;!d[i-j+n])//（i，j）所占列，主，副对角线都未被标记的情况下
        {
            a[i] = j;
            b[j] = 1;
            c[i+j] = 1;
            d[i-j+n] = 1;//打标记
            dfs(i+1);
            b[j] = 0;
            c[i+j] = 0;
            d[i-j+n] = 0;//回溯的时候，需要把标记去掉
        }
    }
}
int main()
{
    scanf(&quot;%d&quot;,&amp;n);
    dfs(1);
    printf(&quot;%d&quot;,total);
    return 0;
}
</code></pre>
<ul>
<li>需要注意的几点性质（关于这个性质可以很容易画图得到）：<br>
1.主对角线上所有点的坐标x+y相等（这里的主对角线表示的是左下到右上）<br>
2.副对角线上所有点的坐标x-y相等（副对角线表示左上到右下）<br>
3.代码中d[i-j+n]，加n的原因是i-j会出现负数，而我们对其加任意大小的数不会改变她表示副对角线的唯一性，至于加n的原因是i-j最大的负数是1-n，所以我们只需要加n即可。</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[二分查找（binary_search）]]></title>
        <id>https://hehelv.github.io//post/er-fen-cha-zhao-binary_search</id>
        <link href="https://hehelv.github.io//post/er-fen-cha-zhao-binary_search">
        </link>
        <updated>2020-01-14T03:50:01.000Z</updated>
        <content type="html"><![CDATA[<h1 id="一个例子">一个例子：</h1>
<p><img src="https://hehelv.github.io//post-images/1578974006618.png" alt=""><br>
根据这个问题很容易想到用一个四重循环来写，通过暴力求解得方法，代码如下：</p>
<pre><code class="language-c++">#include&lt;bits/stdc++.h&gt;
using namespace std;

const int MAX_N = 50;

int main()
{
    int n,m,k[MAX_N];

    bool f = false;//判断是否能找到和为m得四个数字
    scanf(&quot;%d%d&quot;,&amp;n,&amp;m);
    for(int i=0;i&lt;n;i++)
    {
        scanf(&quot;%d&quot;,k[i]);
    }

    for(int a=0;a&lt;n;a++)
    {
        for(int b=0;b&lt;n;b++)
        {
            for(int c=0;c&lt;n;c++)
            {
                for(int d=0;d&lt;n;d++)
                {
                    if(k[a]+k[b]+k[c]+k[d] == m)
                    {
                        f = true;
                    }
                }
            }
        }
    }
    if(f)
        puts(&quot;yes&quot;);
    else 
        puts(&quot;no&quot;);
    return 0;
}

/*
n=3
m=9
k={1,3,5}

no
*/

</code></pre>
<h1 id="更快的方法二分查找">更快的方法：二分查找</h1>
<p>我们发现用四重循环的时间复杂度为O(n<sup>4)，我们利用二分查找来代替最里面那一层的循环，在用二分查找之前需要我们排序，而排序的时间复杂度为O(nlogn),二分查找的时间复杂度为O(logn),代替最里层的循环后，总时间复杂度为O(n</sup>3logn),所以时间复杂度为O(n^3logn)。</p>
<ul>
<li>代码如下：</li>
</ul>
<pre><code class="language-c++">#include&lt;bits/stdc++.h&gt;
using namespace std;

const int MAX_N = 50;
int n,m,k[MAX_N];

bool Binary_Search(int x)
{
    int l=0,r=n;//这里要求r必须等于n，也就是说要多出一个无用的数组位，因为当x=k[n-1]时，l最终会等于n-1，如果r=n-1则会提前退出循环
    while(r-l &gt;= 1)
    {
        int i = (r+l)/2;
        if(k[i] == x) return true;
        else if(k[i] &lt; x) l = i+1;
        else r = i;
    }
    return false;
}

int main()
{
    bool f = false;//判断是否能找到和为m得四个数字

    scanf(&quot;%d%d&quot;,&amp;n,&amp;m);

    for(int i=0;i&lt;n;i++)
    {
        scanf(&quot;%d&quot;,&amp;k[i]);
    }
    sort(k,k+n);//将数组排序

    for(int a=0;a&lt;n;a++)
    {
        for(int b=0;b&lt;n;b++)
        {
            for(int c=0;c&lt;n;c++)
            {
                if(Binary_Search(m-k[a]-k[b]-k[c]))
                    f = true;
            }
        }
    }
    if(f)
        puts(&quot;yes&quot;);
    else 
        puts(&quot;no&quot;);
    return 0;
}

/*
n=3
m=9
k={1,3,5}

no
*/

</code></pre>
<h1 id="再优化">再优化：</h1>
<ul>
<li>再使用了二分查找之后时间复杂度为O(n^3logn),可以利用二分查找进一步的优化，我们可以将四重循环的内两层循环换成二分查找，这时候的式子为k[c]+k[d] = m-k[a]-k[b],我们需要一个数组来保存k[c]+k[d]的所有情况。</li>
<li>这时候的时间复杂度：</li>
<li>1.排序时间复杂度：O(n^2logn)</li>
<li>2.循环加二分查找复杂度：O(n^2logn)</li>
<li>到这里我们可以发现，再去一层循环已经没有意义了，因为此时的数组排序时间复杂度为O(n^3logn).</li>
<li>代码如下：</li>
</ul>
<pre><code class="language-c++">#include&lt;bits/stdc++.h&gt;
using namespace std;

const int MAX_N = 50;
int n,m,k[MAX_N];
int kk[MAX_N*MAX_N];

bool Binary_Search(int x)
{
    int l=0,r=n;//这里要求r必须等于n，也就是说要多出一个无用的数组位，因为但x=k[n-1]时，l最终会等于n-1，如果r=n-1则会提前退出循环
    while(r-l &gt;= 1)
    {
        int i = (r+l)/2;
        if(kk[i] == x) return true;
        else if(kk[i] &lt; x) l = i+1;
        else r = i;
    }
    return false;
}

int main()
{
    bool f = false;//判断是否能找到和为m得四个数字

    scanf(&quot;%d%d&quot;,&amp;n,&amp;m);

    for(int i=0;i&lt;n;i++)
    {
        scanf(&quot;%d&quot;,&amp;k[i]);
    }
    for(int c=0;c&lt;n;c++)
    {
        for(int d=0;d&lt;n;d++)
        {
            kk[c*n+d] = k[c] + k[d];
        }
    }

    sort(kk,kk+n*n);//将数组排序

    for(int a=0;a&lt;n;a++)
    {
        for(int b=0;b&lt;n;b++)
        {
            if(Binary_Search(m-k[a]-k[b]))
                    f = true;
        }
    }
    if(f)
        puts(&quot;yes&quot;);
    else 
        puts(&quot;no&quot;);
    return 0;
}
/*
n=3
m=9
k={1,3,5}

no
*/
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ants问题]]></title>
        <id>https://hehelv.github.io//post/ants-wen-ti</id>
        <link href="https://hehelv.github.io//post/ants-wen-ti">
        </link>
        <updated>2020-01-10T12:56:46.000Z</updated>
        <content type="html"><![CDATA[<h1 id="问题">问题：</h1>
<figure data-type="image" tabindex="1"><img src="https://hehelv.github.io//post-images/1578666147784.png" alt=""></figure>
<ul>
<li>解决思路：</li>
<li>我们想考虑一下如果两只蚂蚁相遇了的情形：</li>
<li><img src="https://hehelv.github.io//post-images/1578666310683.png" alt=""></li>
<li>发现每只蚂蚁都可以看成相遇后仍然不调转方向，因为问题本身不关注个体的蚂蚁，在我们看来所有的蚂蚁都是一样的。</li>
</ul>
<h1 id="代码如下">代码如下：</h1>
<pre><code class="language-c">#include&lt;bits/stdc++.h&gt;
#define MAX_N 100

int max(int x,int y)
{
    if(x-y&gt;0)
        return x;
    else return y;
}

int min(int x,int y)
{
    if(x-y&gt;0)
        return y;
    else return x;
}

int main()
{
    int L,n;
    int minT,maxT;
    int a[MAX_N];  //用来收集每只蚂蚁的信息
    scanf(&quot;%d%d&quot;,&amp;L,&amp;n);
    for(int i=1;i&lt;=n;i++)
    {
        scanf(&quot;%d&quot;,&amp;a[i]);
    }
    for(int i=1;i&lt;=n;i++)
    {
        minT = max(minT,min(a[i],L-a[i]));
        maxT = max(maxT,max(a[i],L-a[i]));
    }
    printf(&quot;min:%d max:%d&quot;,minT,maxT);
    return 0;
}
/*
10 3
2
6
7
min:4 max:8
*/
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[DFS（Depth first search）与BFS（Breadth First Search）]]></title>
        <id>https://hehelv.github.io//post/dfsdepth-first-search</id>
        <link href="https://hehelv.github.io//post/dfsdepth-first-search">
        </link>
        <updated>2020-01-09T09:12:10.000Z</updated>
        <content type="html"><![CDATA[<h1 id="深度优先搜索模型">深度优先搜索模型：</h1>
<pre><code class="language-c">void dfs(int step)
{
    判断边界
    尝试每一种可能 for(i=1;i&lt;=n;i++)
    {
        继续下一步 dfs(step+1);
    }
    返回
}
</code></pre>
<p>对于深度优先搜索，首先要明白当前怎么做，然后直接dfs(step+1)，做下一步即可。</p>
<h1 id="bfs">BFS：</h1>
<p>通常用来解决最小路径，最少操作类问题，</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[快速排序（quicksort）]]></title>
        <id>https://hehelv.github.io//post/kuai-su-pai-xu</id>
        <link href="https://hehelv.github.io//post/kuai-su-pai-xu">
        </link>
        <updated>2020-01-04T07:35:31.000Z</updated>
        <content type="html"><![CDATA[<p><strong>👏这个笔记是看了《啊哈！算法》之后，自己的一些小总结。</strong></p>
<h1 id="快速排序">快速排序：</h1>
<ul>
<li>还是通过一个例子来说明：</li>
<li>有这样一个序列：3，2，1，5，4</li>
<li>要求从小到大排序</li>
<li>快速排序的想法：</li>
<li>1.先找一个基准数，这里默认为第一个数3；</li>
<li>2.然后将比3小的数移到3的左边，比3大的数移到3的右边；</li>
<li>3.后面用递归将3左边和右边的数组在实现一次上述操作。</li>
<li><img src="https://hehelv.github.io//post-images/1578128045451.gif" alt=""></li>
</ul>
<h1 id="代码如下">代码如下：</h1>
<pre><code class="language-c">#include&lt;stdio.h&gt;

int a[101],n;

void quicksort(int left,int right)
	{
		int i,j,t,temp;
		i = left;
		j = right;
		if(i &gt; j)
			return;
		temp = a[left];
		
		while(i!=j)
		{
			while(a[j]&gt;=temp &amp;&amp; i&lt;j)
			{
				j--;
			}
			while(a[i]&lt;=temp &amp;&amp; i&lt;j)
			{
				i++;
			}
			if(i!=j)
			{
				t = a[i];
				a[i] = a[j];
				a[j] = t;
			}
		}

		
		a[left] = a[i];
		a[i] = temp;
		
		quicksort(left,i-1);
		quicksort(i+1,right);
	}

int main()
{
	int i,j;
	scanf(&quot;%d&quot;,&amp;n);
	for(i=1;i&lt;=n;i++)
	{
		scanf(&quot;%d&quot;,&amp;a[i]);
	}
	quicksort(1,n);
	for(i=1;i&lt;=n;i++)
	{
		printf(&quot;%d &quot;,a[i]);
	}
	return 0;
}
/*
示例输出：
5
3 3 1 2 5
1 2 3 3 5

5
3 2 1 5 4
1 2 3 4 5
*/
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[简陋的桶排序，冒泡排序。。。]]></title>
        <id>https://hehelv.github.io//post/algorithm</id>
        <link href="https://hehelv.github.io//post/algorithm">
        </link>
        <updated>2020-01-01T04:02:55.000Z</updated>
        <content type="html"><![CDATA[<p><strong>👏这个笔记是看了《啊哈！算法》之后，自己的一些小总结。</strong></p>
<h1 id="1一个简陋的桶排序">1.一个简陋的桶排序：</h1>
<p>在说桶排序算法时，我们来通过一个例子来大概了解一下桶排序是什么：</p>
<ul>
<li>对8，5，5，3，2这五个数进行从大到小的排序：</li>
<li><strong>想法如下：</strong><br>
1.构建一个数组a[10]，并将其<strong>置零</strong>。<br>
2.对每个出现的数，在数组对应的位置上加一。（出现一次就加一，例子中5出现了两次，那么a[5] = 2）<br>
3.然后将数组倒序输出，如果<strong>a[i] = 0</strong>，则不输出，若不为0则输出i，且输出一次将数组减一。</li>
<li><strong>代码如下：</strong><pre><code class="language-c++">#include &lt;stdio.h&gt;
int main()
{
int a[11],i,j,t;
for(i=0;i&lt;=10;i++)
    a[i]=0; //初始化为0
for(i=1;i&lt;=5;i++) //循环读入5个数
{
    scanf(&quot;%d&quot;,&amp;t); //把每一个数读到变量t中
    a[t]++; //进行计数
}
for(i=0;i&lt;=10;i++) //依次判断a[0]~a[10]
    for(j=1;j&lt;=a[i];j++) //出现了几次就打印几次
        printf(&quot;%d &quot;,i);
return 0;
}
</code></pre>
</li>
<li>但这个方法明显有很大的缺陷：</li>
<li>1.例如当要求排序的数只有3,5,999时，我们却需要创建一个a[1000]的数组，造成了极大的浪费；</li>
<li>2.但要求比较的数不止是整数时，也不能使用这个算法；</li>
<li>所以这里叫做<strong>简陋的桶排序</strong>，真正的桶排序会解决上述问题，这里先不介绍。</li>
</ul>
<h1 id="2冒泡排序">2.冒泡排序：</h1>
<p>依旧从一个例子出发：</p>
<ul>
<li>
<p>对12，35，99，18，76这五个数进行排序：</p>
</li>
<li>
<p>冒泡的基本思想是：<strong>每比较相邻的两个元素，如果他们顺序错误就把他们交换过来。</strong></p>
</li>
<li>
<p>在例子中，将第一位与第二位比较，12小，交换位置，35到第一位，12到第二位，将第二位继续与第三位相比，再次交换顺序，一直进行下去，直到12在第五位（即最后一位），这个数组会变成：35，99，18，76，12（此时最小的数已到达第五位了）</p>
</li>
<li>
<p>再进行第二次冒泡，将第一位与第二位比较，35小，交换顺序，第二位与第三位比较，18小，不交换顺序，第三位与第四位比较，18小，交换顺序，此时数组为：99，35，76，18，12（此时第二小的数已经到达第四位）</p>
</li>
<li>
<p>再对后面的数进行操作，数组最终会变成：99，76，35，18，12</p>
</li>
<li>
<figure data-type="image" tabindex="1"><img src="https://hehelv.github.io//post-images/1578132651155.gif" alt=""></figure>
</li>
<li>
<p><strong>我们来总结一下：</strong></p>
</li>
<li>
<p>1.每进行一次冒泡，就有一个数归位，例如第一次冒泡只能将第五位归位，第二次将第四位归位，后面以此类推。</p>
</li>
<li>
<p>2.对于n个数的排序，我们只需要将n-1个数进行归位。</p>
</li>
<li>
<p>依照这个思想，对于任意n个数的排序，代码如下：</p>
</li>
</ul>
<pre><code class="language-c++">#include&lt;stdio.h&gt;
int main()
{
	int a[100],i,j,t,n;
	scanf(&quot;%d&quot;,&amp;n);     //输入一个数n，表示接下来有n个数
	for(i=1;i&lt;=n;i++)   //循环读入n个数到数组a中
	{
		scanf(&quot;%d&quot;,&amp;a[i]);
	}
    //冒泡排序的核心部分
	for(i=1;i&lt;=n-1;i++)     //n个数排序，只用进行n-1趟
	{
		for(j=1;j&lt;=n-i;j++)
		{
			if(a[j]&lt;a[j+1])
			{
				t = a[j+1]; a[j+1] = a[j]; a[j] = t;    //比较大小并交换
				}	
		}
	}
	for(i=1;i&lt;=n;i++)
		printf(&quot;%d &quot;,a[i]);
	return 0;
}
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hello world]]></title>
        <id>https://hehelv.github.io//post/hello-world</id>
        <link href="https://hehelv.github.io//post/hello-world">
        </link>
        <updated>2020-01-01T03:39:09.000Z</updated>
        <content type="html"><![CDATA[<p>😀<strong>First time write here!</strong></p>
<pre><code>    建Blog的目的就是为了用一些简短的方式把自己学的知识记录下来，以及写些琐碎的想法，其他别无他想，其实能把这两点做好就已经十分满足了。                          ————2020.01.01
</code></pre>
]]></content>
    </entry>
</feed>